{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/libexec/java_home -v 18\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set JAVA_HOME to the Java 18 installation path\n",
    "java_home_path = \"/usr/libexec/java_home -v 18\"  # Adjust this to your Java 18 path\n",
    "os.environ['JAVA_HOME'] = java_home_path\n",
    "\n",
    "# Verify the change\n",
    "print(os.environ['JAVA_HOME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/taehyun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/taehyun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/taehyun/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] JVM DLL not found: /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre/lib/jli/libjli.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvader_lexicon\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# For sentiment analysis\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m okt \u001b[38;5;241m=\u001b[39m \u001b[43mOkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m sia \u001b[38;5;241m=\u001b[39m SentimentIntensityAnalyzer()  \u001b[38;5;66;03m# For sentiment analysis\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python/lib/python3.9/site-packages/konlpy/tag/_okt.py:51\u001b[0m, in \u001b[0;36mOkt.__init__\u001b[0;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, jvmpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_heap_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jpype\u001b[38;5;241m.\u001b[39misJVMStarted():\n\u001b[0;32m---> 51\u001b[0m         \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvmpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_heap_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     oktJavaPackage \u001b[38;5;241m=\u001b[39m jpype\u001b[38;5;241m.\u001b[39mJPackage(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkr.lucypark.okt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m     OktInterfaceJavaClass \u001b[38;5;241m=\u001b[39m oktJavaPackage\u001b[38;5;241m.\u001b[39mOktInterface\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python/lib/python3.9/site-packages/konlpy/jvm.py:64\u001b[0m, in \u001b[0;36minit_jvm\u001b[0;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     61\u001b[0m     jvmpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/lib/jli/libjli.dylib\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m jvmpath\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/lib/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jvmpath:\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartJVM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvmpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Dfile.encoding=UTF8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-ea\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xmx\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_heap_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mclasspath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasspath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mconvertStrings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify the JVM path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python/lib/python3.9/site-packages/jpype/_core.py:247\u001b[0m, in \u001b[0;36mstartJVM\u001b[0;34m(jvmpath, classpath, ignoreUnrecognized, convertStrings, interrupt, *jvmargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m prior \u001b[38;5;241m=\u001b[39m [locale\u001b[38;5;241m.\u001b[39mgetlocale(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m categories]\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Start the JVM\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[43m_jpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvmpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvmargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mextra_jvm_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m               \u001b[49m\u001b[43mignoreUnrecognized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvertStrings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Collect required resources for operation\u001b[39;00m\n\u001b[1;32m    250\u001b[0m initializeResources()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] JVM DLL not found: /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre/lib/jli/libjli.dylib"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer  # For sentiment analysis\n",
    "\n",
    "# Download necessary NLTK packages\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"vader_lexicon\")  # For sentiment analysis\n",
    "\n",
    "# Initialize\n",
    "okt = Okt()\n",
    "sia = SentimentIntensityAnalyzer()  # For sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = set(stopwords.words(\"english\")) | {\"RT\"}\n",
    "stop_words_kr = set([\n",
    "    \"에서\", \"이\", \"의\", \"로\", \"에\", \"과\", \"도\", \"를\", \"으로\", \"한\", \"하다\", \"와\", \"에게\", \"등\", \"으로부터\",\n",
    "    \"이다\", \"하는\", \"안\", \"내\", \"늘\", \"들\", \"아\", \"못\", \"기\", \"거\", \"때\", \"것\", \"저\", \"임\", \"또\", \"더\", \"수\",\n",
    "    \"말\", \"중\", \"나\", \"함\", \"게\", \"해\", \"왜\", \"보고\", \"그\", \"뭐\", \"고\", \"좀\", \"네\", \"이제\", \"없다\", \"아니다\",\n",
    "    \"있다\", \"안되다\", \"같다\", \"대한\", \"그렇다\", \"정말\", \"이번\", \"그냥\", \"때문\", \"진짜\", \"지금\", \"어떻다\", \"그거\",\n",
    "    \"오늘\", \"아마\", \"이렇다\", \"조금\", \"다시\", \"여러\", \"한번\", \"어디\", \"누가\", \"무엇\", \"어떤\", \"대해\", \"이런\",\n",
    "    \"그런\", \"다른\", \"어떻게\", \"모든\", \"우리\", \"하지\", \"있는\", \"마음\", \"정도\", \"지난\", \"이미\", \"앞으로\", \"부터\",\n",
    "    \"사이\", \"역시\", \"대로\", \"이내\", \"가장\", \"더욱\", \"무엇\", \"대해\", \"이후\", \"경우\", \"그대로\", \"다만\", \"만큼\",\n",
    "    \"가지\", \"면서\", \"동안\", \"이후\", \"바로\", \"보다\", \"이나\", \"위해\", \"이다\", \"없다\", \"된다\", \"하고\", \"한테\",\n",
    "    \"까지\", \"따라\", \"면서\", \"비해\", \"서는\", \"로서\", \"로써\", \"에게\", \"에서\", \"이고\", \"인데\", \"처럼\", \"하며\",\n",
    "    \"하면\", \"항상\", \"해도\", \"해야\", \"혹은\", \"혹시\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        tweets = json.load(f)\n",
    "    return tweets\n",
    "\n",
    "def preprocess_tweets(tweets, language=\"english\"):\n",
    "    tokens = []\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    if language == \"english\":\n",
    "        for tweet in tweets:\n",
    "            words = nltk.word_tokenize(tweet.lower())\n",
    "            stemmed_words = [\n",
    "                stemmer.stem(word)\n",
    "                for word in words\n",
    "                if word not in stop_words_en and word.isalpha()\n",
    "            ]\n",
    "            tokens.extend(stemmed_words)\n",
    "\n",
    "    elif language == \"korean\":\n",
    "        for tweet in tweets:\n",
    "            words = okt.pos(tweet, norm=True, stem=True)\n",
    "            tokens.extend(\n",
    "                [\n",
    "                    word\n",
    "                    for word, tag in words\n",
    "                    if tag in [\"Noun\", \"Adjective\"] and word not in stop_words_kr\n",
    "                ]\n",
    "            )\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweets from files (you need to replace '메이플.json' and '오버워치.json' with your actual file names)\n",
    "tweets_maple = load_tweets(\"메이플.json\")\n",
    "tweets_loa = load_tweets(\"오버워치.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentiment_scores\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Preprocess tweets for MapleStory and Overwatch, and then perform sentiment analysis on English tweets\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m tokens_maple_en \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtweets_maple\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlang\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m sentiment_maple_en \u001b[38;5;241m=\u001b[39m analyze_sentiment([tweet[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets_maple \u001b[38;5;28;01mif\u001b[39;00m tweet[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m tokens_loa_en \u001b[38;5;241m=\u001b[39m preprocess_tweets([tweet[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets_loa \u001b[38;5;28;01mif\u001b[39;00m tweet[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36mpreprocess_tweets\u001b[0;34m(tweets, language)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets:\n\u001b[1;32m     12\u001b[0m         words \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(tweet\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m---> 13\u001b[0m         stemmed_words \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m             stemmer\u001b[38;5;241m.\u001b[39mstem(word)\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words_en \u001b[38;5;129;01mand\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha()\n\u001b[1;32m     17\u001b[0m         ]\n\u001b[1;32m     18\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mextend(stemmed_words)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkorean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets:\n\u001b[1;32m     12\u001b[0m         words \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(tweet\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     13\u001b[0m         stemmed_words \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m             stemmer\u001b[38;5;241m.\u001b[39mstem(word)\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words\n\u001b[0;32m---> 16\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstop_words_en\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha()\n\u001b[1;32m     17\u001b[0m         ]\n\u001b[1;32m     18\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mextend(stemmed_words)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkorean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_words_en' is not defined"
     ]
    }
   ],
   "source": [
    "def analyze_sentiment(tweets):\n",
    "    sentiment_scores = [sia.polarity_scores(tweet) for tweet in tweets]\n",
    "    return sentiment_scores\n",
    "\n",
    "# Example usage:\n",
    "# Preprocess tweets for MapleStory and Overwatch, and then perform sentiment analysis on English tweets\n",
    "tokens_maple_en = preprocess_tweets([tweet[\"text\"] for tweet in tweets_maple if tweet[\"lang\"] == \"en\"], \"english\")\n",
    "sentiment_maple_en = analyze_sentiment([tweet[\"text\"] for tweet in tweets_maple if tweet[\"lang\"] == \"en\"])\n",
    "\n",
    "tokens_loa_en = preprocess_tweets([tweet[\"text\"] for tweet in tweets_loa if tweet[\"lang\"] == \"en\"], \"english\")\n",
    "sentiment_loa_en = analyze_sentiment([tweet[\"text\"] for tweet in tweets_loa if tweet[\"lang\"] == \"en\"])\n",
    "\n",
    "# Note: For Korean tweets, consider using a library that supports Korean sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequencies(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# Add here the code to generate word clouds as previously defined\n",
    "# You might also want to display the sentiment analysis results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
